{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Predictive Maintenance - Label Bias"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT LIBRARIES\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer, MinMaxScaler, normalize\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "### READ DATA\n",
    "##### Needs to be ran from the project directory\n",
    "train_measurements = pd.read_csv('../datasets/measurements.csv')\n",
    "train_measurements = train_measurements.sort_values(by=['measurement_time'], ascending=[True])\n",
    "\n",
    "train_failures = pd.read_csv('../datasets/failures.csv')\n",
    "train_failures = train_failures.sort_values(by=['failure_time'], ascending=[True])"
   ]
  },
  {
   "source": [
    "## Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_measurements.measurement_time = pd.to_datetime(train_measurements.measurement_time, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "train_failures.failure_time = pd.to_datetime(train_failures.failure_time)\n",
    "\n",
    "### MERGE NEXT FAILURE TO MEASUREMENTS\n",
    "train_combined = pd.merge_asof(\n",
    "    train_measurements,\n",
    "    train_failures,\n",
    "    left_on='measurement_time',\n",
    "    right_on='failure_time',\n",
    "    by='gadget_id',\n",
    "    direction='forward',\n",
    ")\n",
    "\n",
    "### TRANSFORM COLUMNS\n",
    "train_combined['time_to_fail'] = train_combined['failure_time']-train_combined['measurement_time']\n",
    "train_combined['fail_in_1h'] = np.where(train_combined['time_to_fail']<pd.Timedelta(hours=1), 1, 0)\n",
    "\n",
    "### CALCULATE RUNNING MEASURES\n",
    "train_combined = train_combined.reset_index(drop=True)\n",
    "train_combined = train_combined.sort_values(by=['gadget_id', 'measurement_time'], ascending=[True, True])\n",
    "\n",
    "train_combined['temperature_6h_std'] = train_combined.groupby('gadget_id')['temperature'].rolling(6).std(ddof=0).reset_index(drop=True)\n",
    "train_combined['pressure_6h_mean'] = train_combined.groupby('gadget_id')['pressure'].rolling(6).mean().reset_index(drop=True)\n",
    "\n",
    "train_combined.to_csv('../datasets/train_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify labels\n",
    "X = ['vibration_y', 'pressure_6h_mean', 'temperature_6h_std']\n",
    "y = 'fail_in_1h'\n",
    "cols = X + [y]\n",
    "\n",
    "df_to_split = train_combined.copy()\n",
    "df_to_split = df_to_split.dropna(subset=cols)\n",
    "df_to_split = df_to_split.reset_index(drop=True)\n",
    "\n",
    "##### Create binary bins to \n",
    "binner = KBinsDiscretizer(n_bins=10, encode='onehot-dense', strategy='kmeans')\n",
    "binner.fit(df_to_split[X])\n",
    "arr_bins= binner.transform(df_to_split[X])\n",
    "df_bins = pd.DataFrame(arr_bins)\n",
    "\n",
    "X = list(df_bins.columns)\n",
    "cols = X + [y]\n",
    "\n",
    "df_to_split = pd.concat([df_to_split, df_bins], axis=1)"
   ]
  },
  {
   "source": [
    "### Split balanced dataset randomly into Train and Validation "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(651, 43)\n(327, 43)\n"
     ]
    }
   ],
   "source": [
    "# create test set (environmental dataset)\n",
    "df_test = df_to_split[df_to_split['gadget_id'].isin([5,6])].reset_index(drop=True)\n",
    "\n",
    "# create training, validation set\n",
    "df_to_split = df_to_split[df_to_split['gadget_id'].isin([1,2,3,4])].reset_index(drop=True)\n",
    "\n",
    "print(df_to_split.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training dataset samples:  (456, 43)\nValidation dataset samples:  (195, 43)\n"
     ]
    }
   ],
   "source": [
    "# Use 70% of remaining data set for training\n",
    "df_train = df_to_split.sample(frac = 0.7)\n",
    "\n",
    "# Remaining 30% used for validation test\n",
    "df_validation = df_to_split.drop(df_train.index)\n",
    "\n",
    "\n",
    "df_train = df_train.sort_values(by=['gadget_id', 'measurement_time'], ascending=[True, True])\n",
    "df_validation = df_validation.sort_values(by=['gadget_id', 'measurement_time'], ascending=[True, True])\n",
    "\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_validation = df_validation.reset_index(drop=True)\n",
    "\n",
    "print(\"Balanced\")\n",
    "print(\"--------------\")\n",
    "print(\"Training dataset samples: \", df_train.shape)\n",
    "print(\"Validation dataset samples: \",df_validation.shape)"
   ]
  },
  {
   "source": [
    "### Split unbalanced dataset randomly into Train and Validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(651, 43)"
      ]
     },
     "metadata": {},
     "execution_count": 765
    }
   ],
   "source": [
    "# create csv to manually unbalance data\n",
    "# df_random = df_to_split.sample(frac=1).reset_index(drop=True)\n",
    "# df_random.to_csv('../datasets/train_combined_unbalanced.csv', index=False)\n",
    "\n",
    "df_unbalanced_to_split = pd.read_csv('../datasets/train_combined_unbalanced.csv')\n",
    "df_unbalanced_to_split = df_unbalanced_to_split.sort_values(by=['measurement_time'], ascending=[True])\n",
    "\n",
    "df_unbalanced_to_split.measurement_time = pd.to_datetime(df_unbalanced_to_split.measurement_time, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df_unbalanced_to_split.failure_time = pd.to_datetime(df_unbalanced_to_split.failure_time)\n",
    "\n",
    "df_unbalanced_to_split = df_unbalanced_to_split.reset_index(drop=True)\n",
    "df_unbalanced_to_split = df_unbalanced_to_split.sort_values(by=['gadget_id', 'measurement_time'], ascending=[True, True])\n",
    "\n",
    "df_unbalanced_to_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unbalanced\n--------------\nTraining dataset samples:  (456, 43)\nValidation dataset samples:  (195, 43)\n"
     ]
    }
   ],
   "source": [
    "# Use 70% of remaining data set for training\n",
    "df_train_unbalanced = df_unbalanced_to_split.sample(frac = 0.7)\n",
    "\n",
    "# Remaining 30% used for validation test\n",
    "df_validation_unbalanced = df_unbalanced_to_split.drop(df_train.index)\n",
    "\n",
    "\n",
    "df_train_unbalanced = df_train_unbalanced.sort_values(by=['gadget_id', 'measurement_time'], ascending=[True, True])\n",
    "df_validation_unbalanced = df_validation_unbalanced.sort_values(by=['gadget_id', 'measurement_time'], ascending=[True, True])\n",
    "\n",
    "\n",
    "df_train_unbalanced = df_train_unbalanced.reset_index(drop=True)\n",
    "df_validation_unbalanced = df_validation_unbalanced.reset_index(drop=True)\n",
    "\n",
    "print(\"Unbalanced\")\n",
    "print(\"--------------\")\n",
    "print(\"Training dataset samples: \", df_train_unbalanced.shape)\n",
    "print(\"Validation dataset samples: \",df_validation_unbalanced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Training, Validation and Test - SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "validation accuracy 0.723\ntest accuracy 0.783\n"
     ]
    }
   ],
   "source": [
    "### PREDICTION PARAMETERS\n",
    "w0 = 1\n",
    "w1 = 8\n",
    "pos_label = 1\n",
    "\n",
    "### SVM\n",
    "svm = SVC(\n",
    "    class_weight={0:w0, 1:w1},\n",
    "    C=1,\n",
    "    random_state=42,\n",
    "    kernel='linear'\n",
    ")\n",
    "# fit model\n",
    "svm.fit(df_train[X], df_train[y])\n",
    "\n",
    "# make prediction on validation set\n",
    "val_pred = svm.predict(df_val_test[X])\n",
    "\n",
    "accuracy = accuracy_score(df_val_test['fail_in_1h'], val_pred )\n",
    "print(\"validation accuracy\", round(accuracy,3))\n",
    "\n",
    "# make prediction on test set\n",
    "test_pred = svm.predict(df_test[X])\n",
    "\n",
    "accuracy2 = accuracy_score(df_test['fail_in_1h'], test_pred )\n",
    "print(\"test accuracy\", round(accuracy2, 3))"
   ]
  }
 ]
}