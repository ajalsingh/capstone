\chapter{Conclusion}
In this capstone project, we have discussed the importance of reliability in Artificial Intelligence 
and especially so in high-risk applications.
Specifically, the reliability factors of label bias and environmental datashift, suitable algorithm selection,
and reward hacking were investigated and recommendations provided.
To summarise, we first explored bias and how it can arise within datasets and deployment environment.
Bias was then replicated through experimentation on a predictive maintenance application. 
Proposed solutions were split into two categories; pre-processing and post-processing.
Pre-processing methods are primarily concerned with data collection and preventing the underrepresentation of groups.
Reactive and proactive methods were also discussed.
Post-processing approaches are also available, but it should be noted that it is almost impossible remove bias from data altogether.

Secondly, suitable algorithm selection factors were studied. The goal was to understand how algorithms can be made more
robust for all applications. Therefore, reliability and performance variables for algorithms were identified and discussed.
Comparison between our experimentation on a predictive maintenance dataset and various other studies proved that ensemble methods 
are the better options than standalone algorithms, which tend to vary by application. 
Furthermore, the reliability of algorithms and algorithm selection can also be improved with hyperparameter optimisation and proper consideration of 
evaluation metrics based on the use case.

Finally, a systematic literature review on reward hacking highlighted the importance of AI safety when powerful agents
are involved. Reward hacking can be caused by issues such as RF tampering and system complexity. 
It is also common for a number of these identified issues to occur in combination hence increasing the need for controls.
A number of methods, such as reward strategies and algorithmic approaches, have been offered to resolve reward hacking.