\chapter{Conclusion}
In this capstone project we have discussed the importance of reliability in Artificial Intelligence 
and especially so in high-risk applications.
Specifically, the reliability factors of label bias and environmental datashift, suitable algorithm selection,
and reward hacking were investigated.
We first explored bias and and how it can arise within datasets and deployment environment. r
Bias was then replicated through experimentation on a predictive maintenance dataset. 
Proposed solutions were split into two categories; pre-processing and post-processing.
Pre-processing methods are mostly concerned with data collection and preventing under-representation of groups.
Reactive and proactive methods were also discussed..
Post-processing approaches are also availeble but it should be noted that it is almost impossible to completely remove 
bias from data.

Secondly, suitable algorithm selection factors were studied. The goal was to understand how algorithms can be made more
robust for all applications. Therefore, reliability and performance variables for algorithms were identified and discussed.
Comparison between our experimentation on a predictive maintenance dataset and various other studies proved that ensemble methods 
are the better options than standalone algorithms which tend to vary by application. 
Reliability of algorithm selection can also be improved with hyperparameter optimisation and proper consideration of 
evaluation metrics based on the use case.

Finally, systematic literature review on reward hacking highlighted the importance of AI safety when powerful agents
are involved. Rewarf hacking can be caused by issues such as RF tampering and system complexity. 
It is also common for a number of these identified issues to occur in combination hence increasing the need for controls.
A number of methods, such as reward strategies and algorithmic approaches, have been offered to resolve reward hacking.