{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT LIBRARIES\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer, MinMaxScaler, normalize\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "### READ DATA\n",
    "##### Needs to be ran from the project directory\n",
    "train_measurements = pd.read_csv('../../datasets/IoT/measurements.csv')\n",
    "train_measurements = train_measurements.sort_values(by=['measurement_time'], ascending=[True])\n",
    "\n",
    "train_failures = pd.read_csv('../../datasets/IoT/failures.csv')\n",
    "train_failures = train_failures.sort_values(by=['failure_time'], ascending=[True])"
   ]
  },
  {
   "source": [
    "## Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_measurements.measurement_time = pd.to_datetime(train_measurements.measurement_time, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "train_failures.failure_time = pd.to_datetime(train_failures.failure_time)\n",
    "\n",
    "### MERGE NEXT FAILURE TO MEASUREMENTS\n",
    "train_combined = pd.merge_asof(\n",
    "    train_measurements,\n",
    "    train_failures,\n",
    "    left_on='measurement_time',\n",
    "    right_on='failure_time',\n",
    "    by='gadget_id',\n",
    "    direction='forward',\n",
    ")\n",
    "\n",
    "### TRANSFORM COLUMNS\n",
    "train_combined['time_to_fail'] = train_combined['failure_time']-train_combined['measurement_time']\n",
    "train_combined['fail_in_1h'] = np.where(train_combined['time_to_fail']<pd.Timedelta(hours=1), 1, 0)\n",
    "\n",
    "### CALCULATE RUNNING MEASURES\n",
    "train_combined = train_combined.reset_index(drop=True)\n",
    "train_combined = train_combined.sort_values(by=['gadget_id', 'measurement_time'], ascending=[True, True])\n",
    "\n",
    "train_combined['temperature_6h_std'] = train_combined.groupby('gadget_id')['temperature'].rolling(6).std(ddof=0).reset_index(drop=True)\n",
    "train_combined['pressure_6h_mean'] = train_combined.groupby('gadget_id')['pressure'].rolling(6).mean().reset_index(drop=True)\n",
    "\n",
    "train_combined.to_csv('../../datasets/IoT/train_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(978, 43)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# specify labels\n",
    "X = ['vibration_y', 'pressure_6h_mean', 'temperature_6h_std']\n",
    "y = 'fail_in_1h'\n",
    "cols = X + [y]\n",
    "\n",
    "df_to_split = train_combined.copy()\n",
    "df_to_split = df_to_split.dropna(subset=cols)\n",
    "df_to_split = df_to_split.reset_index(drop=True)\n",
    "\n",
    "##### Create binary bins to \n",
    "binner = KBinsDiscretizer(n_bins=10, encode='onehot-dense', strategy='kmeans')\n",
    "binner.fit(df_to_split[X])\n",
    "arr_bins= binner.transform(df_to_split[X])\n",
    "df_bins = pd.DataFrame(arr_bins)\n",
    "\n",
    "X = list(df_bins.columns)\n",
    "cols = X + [y]\n",
    "\n",
    "df_to_split = pd.concat([df_to_split, df_bins], axis=1)\n",
    "df_to_split.shape"
   ]
  },
  {
   "source": [
    "## Create differing environmental data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle complete dataset\n",
    "df_to_split = df_to_split.sample(frac=1).reset_index(drop=True).copy()\n",
    "\n",
    "# Use 20% of complete dataset as a common test dataset for both biased and unbiased\n",
    "df_test1 = df_to_split.sample(frac = 0.2).copy()\n",
    "df_train_val = df_to_split.drop(df_test1.index).copy()\n",
    "\n",
    "df_test2 = df_test1.copy()\n",
    "for i in df_test2.index:\n",
    "    df_test2.at[i, 'pressure_6h_mean'] += 100\n"
   ]
  },
  {
   "source": [
    "## Datashift Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# train consists of gadgets 1 -4\n",
    "df_train_bias = df_to_split[df_to_split['gadget_id'].isin([1,2,3,4])].reset_index(drop=True).copy()\n",
    "\n",
    "# test consists of gfadgets 5 and 6\n",
    "df_validation_bias = df_to_split[df_to_split['gadget_id'].isin([5,6])].reset_index(drop=True).copy()\n",
    "\n",
    "for i in df_train_bias.index:\n",
    "    df_train_bias.at[i, 'temperature_6h_std'] = 1\n",
    "\n",
    "# # Validation consists of 70% of df_train dataset\n",
    "# # First shuffle df_train\n",
    "# df_train_val_bias = df_train_val_bias.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# # Use 70% of remaining data set for training\n",
    "# df_train_bias = df_train_val_bias.sample(frac = 0.7)\n",
    "\n",
    "# # Remaining 30% used for validation test\n",
    "# df_validation_bias = df_train_val_bias.drop(df_train_bias.index)\n",
    "\n",
    "# Reorder datasets by measurement time and gadget id\n",
    "df_train_bias = df_train_bias.sort_values(by=['gadget_id', 'measurement_time'], ascending=[True, True])\n",
    "df_validation_bias = df_validation_bias.sort_values(by=['gadget_id', 'measurement_time'], ascending=[True, True])\n",
    "# df_test2 = df_test2.sort_values(by=['gadget_id', 'measurement_time'], ascending=[True, True])\n",
    "\n",
    "df_train_bias = df_train_bias.reset_index(drop=True)\n",
    "df_validation_bias = df_validation_bias.reset_index(drop=True)\n",
    "# df_test2 = df_test2.reset_index(drop=True)\n",
    "\n",
    "print(\"Datashift Dataset\")\n",
    "print(\"--------------\")\n",
    "print(\"Training dataset samples: \", df_train_bias.shape)\n",
    "print(\"Validation dataset samples: \",df_validation_bias.shape)\n",
    "print(\"Test dataset samples: \",df_test2.shape)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Datashift Dataset\n--------------\nTraining dataset samples:  (651, 43)\nValidation dataset samples:  (327, 43)\nTest dataset samples:  (196, 43)\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## Balanced Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Balanced Dataset\n--------------\nTraining dataset samples:  (626, 43)\nValidation dataset samples:  (156, 43)\nTest dataset samples:  (196, 43)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle complete dataset\n",
    "# df_train_val_test_balanced = df_to_split.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_train_balanced = df_train_val.sample(frac = 0.8)\n",
    "df_validation_balanced = df_train_val.drop(df_train_balanced.index)\n",
    "\n",
    "\n",
    "\n",
    "# # 60-20-20 split for train-validation-test repectively\n",
    "# df_train_balanced = df_train_val_test_balanced.sample(frac = 0.7)\n",
    "# df_val_test_balanced = df_train_val_test_balanced.drop(df_train_balanced.index)\n",
    "\n",
    "# # half the remaining remaining 40% of dataset\n",
    "# df_validation_balanced = df_val_test_balanced.sample(frac = 0.7)\n",
    "# df_test_balanced = df_val_test_balanced.drop(df_validation_balanced.index)\n",
    "\n",
    "# Reorder datasets by measurement time and gadget id\n",
    "df_train_balanced = df_train_balanced.sort_values(by=['gadget_id', 'measurement_time'], ascending=[True, True])\n",
    "df_validation_balanced = df_validation_balanced.sort_values(by=['gadget_id', 'measurement_time'], ascending=[True, True])\n",
    "df_test1 = df_test1.sort_values(by=['gadget_id', 'measurement_time'], ascending=[True, True])\n",
    "\n",
    "df_train_balanced = df_train_balanced.reset_index(drop=True)\n",
    "df_validation_balanced = df_validation_balanced.reset_index(drop=True)\n",
    "df_test1 = df_test1.reset_index(drop=True)\n",
    "\n",
    "print(\"Balanced Dataset\")\n",
    "print(\"--------------\")\n",
    "print(\"Training dataset samples: \", df_train_balanced.shape)\n",
    "print(\"Validation dataset samples: \",df_validation_balanced.shape)\n",
    "print(\"Test dataset samples: \",df_test1.shape)"
   ]
  },
  {
   "source": [
    "# Training, Validation and Test - SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREDICTION PARAMETERS\n",
    "w0 = 1\n",
    "w1 = 8\n",
    "pos_label = 1\n",
    "\n",
    "### SVM\n",
    "svm = SVC(\n",
    "    class_weight={0:w0, 1:w1},\n",
    "    C=1,\n",
    "    random_state=42,\n",
    "    kernel='linear'\n",
    ")\n",
    "\n",
    "### RANDOM FOREST MODEL\n",
    "random_forest = RandomForestClassifier(\n",
    "    min_samples_leaf=7,\n",
    "    random_state=45,\n",
    "    n_estimators=50,\n",
    "    class_weight={0:w0, 1:w1}\n",
    ")"
   ]
  },
  {
   "source": [
    "## Balanced"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "validation accuracy 0.769\nvalidation precision 0.357\nvalidation recall 1.0\ntest accuracy 0.74\ntest precision 0.254\ntest recall 0.944\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "random_forest.fit(df_train_balanced[X], df_train_balanced[y])\n",
    "\n",
    "# make prediction on validation set\n",
    "val_pred_balanced = random_forest.predict(df_validation_balanced[X])\n",
    "\n",
    "accuracy_val_balanced = accuracy_score(df_validation_balanced['fail_in_1h'], val_pred_balanced )\n",
    "precision_val_balanced = precision_score(df_validation_balanced['fail_in_1h'], val_pred_balanced, zero_division=0, pos_label=pos_label)\n",
    "recall_val_balanced = recall_score(df_validation_balanced['fail_in_1h'], val_pred_balanced, pos_label=pos_label)\n",
    "\n",
    "print(\"validation accuracy\", round(accuracy_val_balanced,3))\n",
    "print(\"validation precision\", round(precision_val_balanced, 3))\n",
    "print(\"validation recall\", round(recall_val_balanced, 3))\n",
    "\n",
    "# # make prediction on test set\n",
    "test_pred_balanced = random_forest.predict(df_test1[X])\n",
    "\n",
    "accuracy_test_balanced = accuracy_score(df_test1['fail_in_1h'], test_pred_balanced )\n",
    "precision_test_balanced = precision_score(df_test1['fail_in_1h'], test_pred_balanced, zero_division=0, pos_label=pos_label)\n",
    "recall_test_balanced = recall_score(df_test1['fail_in_1h'], test_pred_balanced, pos_label=pos_label)\n",
    "\n",
    "print(\"test accuracy\", round(accuracy_test_balanced, 3))\n",
    "print(\"test precision\", round(precision_test_balanced, 3))\n",
    "print(\"test recall\", round(recall_test_balanced, 3))"
   ]
  },
  {
   "source": [
    "## Biased"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "validation accuracy 0.783\nvalidation precision 0.311\nvalidation recall 1.0\ntest accuracy 0.74\ntest precision 0.261\ntest recall 1.0\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "random_forest.fit(df_train_bias[X], df_train_bias[y])\n",
    "\n",
    "# make prediction on validation set\n",
    "val_pred_biased = random_forest.predict(df_validation_bias[X])\n",
    "\n",
    "accuracy_val_biased = accuracy_score(df_validation_bias['fail_in_1h'], val_pred_biased )\n",
    "precision_val_biased = precision_score(df_validation_bias['fail_in_1h'], val_pred_biased, zero_division=0, pos_label=pos_label)\n",
    "recall_val_biased = recall_score(df_validation_bias['fail_in_1h'], val_pred_biased, pos_label=pos_label)\n",
    "\n",
    "print(\"validation accuracy\", round(accuracy_val_biased,3))\n",
    "print(\"validation precision\", round(precision_val_biased, 3))\n",
    "print(\"validation recall\", round(recall_val_biased, 3))\n",
    "\n",
    "# make prediction on test set\n",
    "test_pred_biased = random_forest.predict(df_test2[X])\n",
    "\n",
    "accuracy_test_biased = accuracy_score(df_test2['fail_in_1h'], test_pred_biased )\n",
    "precision_test_biased = precision_score(df_test2['fail_in_1h'], test_pred_biased, zero_division=0, pos_label=pos_label)\n",
    "recall_test_biased = recall_score(df_test2['fail_in_1h'], test_pred_biased, pos_label=pos_label)\n",
    "\n",
    "print(\"test accuracy\", round(accuracy_test_biased, 3))\n",
    "print(\"test precision\", round(precision_test_biased, 3))\n",
    "print(\"test recall\", round(recall_test_biased, 3))"
   ]
  }
 ]
}