%% Suitable Algorithm Selection

\chapter{Suitable Algorithm Selection}
Unreliable Machine Learning models can be the result of inadequate model assumptions where inappropriate or unsuitable algorithm/s have been used.
The appropriacy of an algorithm is dependant on multiple factors. One such factor is the level of supervision required, which in turn is dependant on the amount and type of data available.
Another key factor is the use case of the model and its intended outcomes. Generally model parameters are curated for specific applications and will differ to other use cases.
Therefore, it is important to make use of inductive bias\cite{saria2019tutorial} to when developing reliable models.

\section{Dataset \& Preprocessing}
The predictive maintenance dataset will be used again to classify failures of an ioT gadget.
During one week, maintenance data was collected from six devices every hour for 168 hrs.
Therefore, this data set contains 1008 rows of data. 
Each cycle of data reading contains the following measurements: 

\begin{table}[ht]
    \begin{center}
        \caption{Measurements Dataset} 
        \begin{tabular}{ l|l } 
         \toprule
         \textbf{Measurement} & \textbf{Description} \\  [0.5ex] 
         \midrule
         \textbf{Measurement Time} & Time \\
         \textbf{Gadget ID} & Device number \\
         \textbf{Vibration x sensor} & Horizontal vibration \\ 
         \textbf{Vibration y sensor} & Vertical vibration \\ 
         \textbf{pressure sensor} & Hose pressure \\
         \textbf{Temperature sensor} & Internal temperature \\
         \bottomrule
        \end{tabular}
    \end{center}
\end{table}

The failures dataset contains the precise times each gadget failed. 
During the course of the week, 105 failures were recorded.
Device failure is to be classified when the time remaining to device failure is less than one hour.

This dataset has been split into two datasets for training and testing respectively.
The training dataset will compromise of all data collected from gadget IDs 1-4, leaving data from gadgets 5 and 6 for the test set.
This will ensure the trained models are tested on completely new data. 

For more information on the dataset and use case, please see []
\hyperlink{gitub repo}{https://github.com/Unikie/predictive-maintenance-tutorial}

\section{Algorithms}
The following section shortlists and describes algorithms which can be used to train a supervised model. 
Also discussed are the factors affecting the performance and reliability of these algorithms.

\subsection{Support Vector Machines}
Support Vector Machines (SVM) is a common supervised machine learning algorithm for both classification and regression tasks.
A key attribute of SVMs is its high accuracy and precision in thge segregation of classes.
SVMs create $n$-dimensional hyperplanes to segregate datapoints into $n$ number of classes/groups. 
The algorithm aims to achieve the maximum margin between support vecotrs(closest points), i.e. maximise the minimum margin.

In the case where two classes can be linearly seperated, we consider the following as a representation of a dataset, $S$.
\begin{equation}
    S = \Big\{x_i \in \R^{1 \times p}, y_i \in \{-1, 1\}\Big\}^n_{i=1}
\end{equation}

The values $\{-1, 1\}$ represent two classes of data, $A$ and $B$,
\begin{equation}
    y_i = \begin{cases}
        1, & \text{if $i$} \text{-th sample} \in  A\\
        -1, & \text{if $i$} \text{-th sample} \in  B.
    \end{cases}
\end{equation}

The hyperplane can then be defined as $F_0$ in $\R^D$ space as,
\begin{equation}
    F_0 = \big \{x|f(x) = x \beta + \beta_0 = 0 \big \} 
\end{equation}  
where, 
$\beta \in \R^D$ with norm $||\beta|| = 1$

For a new sample $x^{new}$ which is not within dataset $S$, we can detemine a classification as,
\begin{equation}
    y_{new} = \begin{cases}
        1 \text{(Class A) }, & \text{if } f(x^{new}) > 0 \\
        -1 \text{(Class B) }, & \text{if } f(x^{new}) < 0
    \end{cases}
\end{equation}

The performance of SVMs can be further improved by implementing kernel methods. Some popular kernel functions are:
\begin{figure}[h]
    \includegraphics[scale=0.7]{SVM_kernel.png}
    \centering
    % \caption{Random Forest Algorithm \cite{chen}}
    \label{fig:kernel}
\end{figure}

A linear kernel will be used in this experiment.

%ref
% https://ieeexplore-ieee-org.ezproxy.lib.uts.edu.au/stamp/stamp.jsp?tp=&arnumber=6653952&tag=1
% https://pureadmin.qub.ac.uk/ws/files/17844756/machine.pdf


\subsection{k-Nearest Neighbours}
k-Nearst Neighbours (k-NN) is a simple supervised machine learning algorithm used in both classification and regression problems. 
This approach classifies objects based on the compuatational distances or similarities between samples/values.
The k-NN algorithm only requires tuning of a single parameter, $k$, which represents the amount of nearest samples within the neighbourhood.
The choice of $k$ will affect the algorithm's performance where a value too small would create higher variance hence resulting in less stability.
A larger $k$ value will produce higher bias resulting in lower precision. 

After the number of neighbours, $k$, has been selected, the distances between the query data point, $x_q$, and an arbitrary data point, $x_i$ are to be determined.
Most commonly used is the Euclidean distance (\ref{Euclidean}), however Manhattan distance (\ref{Manhattan}) may also be applied.

\begin{equation} \label{Euclidean}
    d(x_q,x_i) = \sqrt{\sum_{i=1}^{m} (x_q - x_i)^2}
\end{equation}
\begin{equation} \label{Manhattan}
    d(x_q,x_i) = \sum_{i=1}^{m} \lvert x_r - x_i \rvert
\end{equation}

The resulting values are then sorted by distance from smallest to largest and the first $k$ entries are selected. 
In classification problems, the mode of $k$ labels is returned, while the mean of $k$ labels is returned in regression problems.
The choice of parameter $k$ can impact the models reliability. Too small a value for $k$ will result in higher variance while a value too high will increase bias. 
This can be problematic when used on noisy datasets. Cross validation can be used to determine the correct $k$ value.

% references:
% https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761
% https://www.listendata.com/2017/12/k-nearest-neighbor-step-by-step-tutorial.html
% http://www.lkozma.net/knn2.pdf
% https://ieeexplore-ieee-org.ezproxy.lib.uts.edu.au/stamp/stamp.jsp?tp=&arnumber=7456223

\subsection{Random Forest}
Random Forest is an ensemble machine learning method which creates multiple random decision trees and combines their respective votes (classification) or averages (regression) to improve prediction accuracy and fitting.

% \enlargethispage{3\baselineskip}
\begin{figure}[h]
    \includegraphics[scale=0.7]{random_forest.png}
    \centering
    % \caption{Random Forest Algorithm \cite{chen}}
    \label{fig:RF}
\end{figure}

Add more stuff about reliability/performance factors

% Ref:
% https://www.math.mcgill.ca/yyang/resources/doc/randomforest.pdf

\subsection{Neural Networks}
Some filler text for the time being.

\section{Results}
Building on top of the predictive maintenance project [], we train models using the following algorithms and test for Precision, Recall, Accuracy, and AUC scores.
The results are depicted in Figures \ref{fig:Metric plot}, \ref{fig:AUC}, and Tables \ref{table:Metrics}, \ref{table:Conf}
\begin{itemize}
    \item Random Forests (RF)
    \item Logarithmic Regression
    \item Linear Regression
    \item k-Nearest Neighbours (knn)
    \item Neural Networks (nn)
    \item Support Vector Machines (SVM)
    \item Naive Bayes
    \item Stochastic Gradient Descent (SGD/clf)
\end{itemize}

\begin{figure}[H]
    \includegraphics[scale=0.9]{results.png}
    \centering
    \caption{Precision, Recall and Accuracy Scores}
    \label{fig:Metric plot}
\end{figure}

\begin{figure}[H]
    \includegraphics[scale=0.9]{AUC.png}
    \centering
    \caption{Area Under the Curve}
    \label{fig:AUC}
\end{figure}

\begin{table}[h]
    \caption{PDM Classification Metrics}
    \label{table:Metrics}
    \begin{tabular}{lrrrrr}
        \toprule
            \textbf{Algorithm} &  \textbf{Precision} &  \textbf{Recall} &  \textbf{Accuracy} &       \textbf{AUC} &        \textbf{F1} \\
        \midrule
        \textbf{Random Forest} &   0.310680 &  \cellcolor{lightgray} 1.0000 &  0.782875 &  \cellcolor{lightgray}0.879661 &  \cellcolor{lightgray}0.474074 \\
        \textbf{Logarithmic Regression} &   0.300000 &  0.9375 &  0.779817 &  0.850106 &  0.454545 \\
        \textbf{Linear Regression} &   0.000000 &  0.0000 &  \cellcolor{lightgray}0.902141 &  0.500000 &  0.000000 \\
        \textbf{k Nearest Neighbours} &   0.285714 &  0.0625 &  0.892966 &  0.522775 &  0.102564 \\
        \textbf{Neural Network} &   \cellcolor{lightgray}0.363636 &  0.1250 &  0.892966 &  0.550636 &  0.186047 \\
        \textbf{SVM} &   0.303030 &  0.9375 &  0.782875 &  0.851801 &  0.458015 \\
        \textbf{Naive Bayes} &   0.303030 &  0.9375 &  0.782875 &  0.851801 &  0.458015 \\
        \textbf{CLF} &   0.104869 &  0.8750 &  0.256881 &  0.532415 &  0.187291 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h]    
    \centering
    \caption{PDM Confusion Matrix}
    \label{table:Conf}
    \begin{tabular}{lrrrr}
        \toprule
        \textbf{Algorithm} &   \textbf{TP} &   \textbf{FP} &  \textbf{FN} &  \textbf{TN} \\
        \midrule
        \textbf{Random Forest} &  224 &   71 &   0 &  32 \\
        \textbf{Logarithmic Regression} &  225 &   70 &   2 &  30 \\
        \textbf{Linear Regression} &  295 &    0 &  32 &   0 \\
        \textbf{k Nearest Neighbours} &  290 &    5 &  30 &   2 \\
        \textbf{Neural Network} &  288 &    7 &  28 &   4 \\
        \textbf{SVM} &  226 &   69 &   2 &  30 \\
        \textbf{Naive Bayes} &  226 &   69 &   2 &  30 \\
        \textbf{CLF} &   56 &  239 &   4 &  28 \\
        \bottomrule
    \end{tabular}
\end{table}

In Table \ref{table:Metrics}, highlighted are the highest comparison scores between tested algorithms.
The ensemble method Random Forests is evidently the most suited method to this predictive maintenace application.
Linear Regression delivered the highest accuracy meaning it was able to make correct predictions the best. 
However, it did not perform well in the other metrics. This highlights that algorithm evaluation cannot be determined based on accuracy alone.

To ensure reliability and robustness, the goals of the use case should be considered and may contradict performance metrics.
In this scenario, a True Positive (TP) case where the predicted failures are actual failures should take precedence as this is the money saving factor. 
A False Negative (FN) case occurs when the model predicts a non-failure but in reality a failure has occured. This would result in large financial penalties for the company due to downtime.
Therefore, the highest amount of TP cases and lowest amount of FN cases are desired.

\section{Further Research}

A seperate study compares existing resgression based machine learning algorithms on a similar but more complex predictive maintenance application,
\textit{'Prediction of Remaining Useful Lifetime (RUL) of Turbofan Engine using Machine Learning'} \cite{RUL}.

During this study, models were trained on a dataset obtained by NASA's data repository, where turbofan jet engines are run until failure.
During operation 21 sensor measurements are recorded against a time series per engine. Training and evaluation occurs on four datasets, each containing data from 100-250 engines. 
This dataset is much more complex than the one used in the previous experiments,
hence it is expected to see much more variance in accuracy of the tested algorithms. 

Listed below are machine learning algorithms used to predict RUL in this study:
\begin{itemize}
    \item Linear Regression
    \item Decision tree
    \item Support Vector Machine
    \item Random Forest
    \item K-Nearest Neighbours
    \item The K Means Algorithm
    \item Gradient Boosting Method
    \item AdaBoost
    \item Deep Learning
    \item Anova 
\end{itemize}
\bigskip

Like the Random Forest method, Gradien Boosting, AdaBoost, and Anova are also ensemble methods.


\begin{table}[h]
    \bigskip
    \caption{RUL Algorithms Root Mean Square Error values\cite{RUL}}
    \begin{figure}[H]
        \includegraphics[scale=0.4]{RUL_results.png}
        \centering
    \end{figure}
    \label{table:RMSE}
\end{table}

\begin{figure}[H]
    \caption{RUL Algorithms Root Mean Square Error Plots per data set\cite{RUL}}
    \includegraphics[scale=0.4]{RUL_RMSE.png}
    \label{fig:RMSE Plots}
\end{figure}

Table \ref{table:RMSE} and Figure \ref{fig:RMSE Plots} displays the RMSE values of all tested algorithms across all four datasets.
As can clearly be seen, the complexity of this problem results in fluctuation of evaluation scores unlike our previous classification experiment.
RMSE is a measure of the concentration of data around the line of best fit. Therefore, an algorithm with the lowest RMSE value is desired.
In this application, the random Forest algorithm produced the lowest error value.

When comparing these results to those of our previous classification experiement, it is clear that enseble methods perform the best, specifically, Random Forest. 
It should also be noted that SVM outperformed knn in the classification experiement but performed worse in the RUL regression study.


\bigskip
\hl{add another study}

\section{Recommendations}
Above sections highlight the importance of correct model spcifications. Previous assumptions and expectations may not always be correct.

\hl{Use ensemble methods and hyperparameter tuning.

It is also important to select appropriate evaluation metrics based on the model type and application.
Assign a weight or cost to each output of the confusion matrix

Research specific use case scenario to find suitable algorithms through proven research}
% https://pdf.sciencedirectassets.com/271506/1-s2.0-S0957417417X00109/1-s2.0-S0957417417302397/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEF4aCXVzLWVhc3QtMSJGMEQCIC53FsqmSpjp%2BQZj9Uvlsnq4pWNYncx8FF2OQ23FmKgkAiAcXvDwMGwRjH0UYxFOV3NaELWAqNC04I378835HV8Ddyq9Awi3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAMaDDA1OTAwMzU0Njg2NSIMYqA70v0TzrNak19iKpEDBvqMBBuWJ0aYNJouVwXQ35CXZg6ARpd5zZsgttN8QGD%2B3j4NV2v289xxiDQQNfltw6POyIApxTgYr6jp4s5S5FwYqdYy6xIZDl7XbC7IY7chzx3bV5bs%2BdRhZcW8WJPnHAjuH6yC6%2BtUY8e7nk5k2WYsdLfkV8Tk2qYpnSqkjyK1cCTM55SOwbgcNJ0w%2Fpo8PJq3mdzi8kI9fD9DFv8wVsDUs0dRXp2jPtTTduTIhNwGa4JgLTh1xplV23kKewbIalHtdNFiqPZB%2Bbr%2FITjQSbWAlwvorEiq31BbESWXfTIIlxQdixRt20XOCypDeG2RrsBe3jPt3t3DBVNF4%2BOkNE2%2BaWmbtgcekVFXZEsTpRRMirynYAJ3FKpj%2BRck%2FKKxUQ9UjqhrMkQrDvXfcqo5QrWlvI4uGPiNoec5Z%2F6bRMXdDgDWP%2FvXQ6Q86YA%2BOLvbiNWgQTfmUFYT8fFgsljtVqiov7W%2BRNWA2Vj%2F%2BKsxtdc7Ok%2BOhxSE%2F9JS%2BSemJuBxJmSlbQqOr8DPQHYlp6h4YxMwl8DPgwY67AGgOpD6YmdbpnxG6nEHk1pw57wlOHkaF2iNo4FC9KM8jegDXaKv0LtK3bBsAigrskq4QKpZXpaAopbnKphKhfoOYrdmgIm7Ba0V44nNPSe46I5%2FsBikWMh3ZQ6s7lqBT1EQa4ds5NQAhi26Vf02PMgLXLk67acqUys0Go%2FKgQv2reuep1GgeOilitWRdjJwkwe07XUnJbNGMMcYNVvqgn%2FikKG4gZzshEt142pOV5N2lNfJbn7GleRVzP6u5NW1NwKmnbDm50JZ5iGgFV3pOpErKyuMC8JoEc3vDhF35jJ6tITkIHuvGvxGACX4iw%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210412T063232Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6KJJLFFB%2F20210412%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=3a49f5dce9775617b98bcc583609d05d8482af9edf219e50219977b74ba83b62&hash=8a44df99adabed4a00d5099a66fae6484d6654bda9d1402f88f056bc364a2330&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0957417417302397&tid=spdf-75df757c-2550-4170-8694-4099a14ded3c&sid=c7d6cc998d9f344cac299b155f55848b59cbgxrqa&type=client
% https://www-sciencedirect-com.ezproxy.lib.uts.edu.au/science/article/pii/S0360835219304838?via%3Dihub
