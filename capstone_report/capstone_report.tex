\documentclass[a4paper,12pt]{report}

\usepackage[english]{babel}
\usepackage{blindtext}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{lscape}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subfig}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{index}
\usepackage{float}
\usepackage{titlesec}
\usepackage{cite}
\usepackage{relsize}
\usepackage{pdflscape}
\usepackage{cite}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage[parfill]{parskip}
\usepackage{hyperref}
\usepackage{color,soul}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\setlength{\footskip}{45pt}
\setcounter{secnumdepth}{3}
\newcommand{\R}{\mathbb{R}}

\graphicspath{ {./images/} }
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\title{\Huge{\textbf{Improving the Reliability of Artificial Intelligence}}\\
\vspace*{0.3cm}
\Large{Capstone Report}}

\titleformat{\chapter}[display]
    {\normalfont\bfseries}{}{0pt}{\huge}\author{Ajal Singh - 12621189\\
    Supervisor: Diep Nguyen}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
    \thispagestyle{empty}
\listoffigures
    \thispagestyle{empty}
\listoftables
    \thispagestyle{empty}


%% Engineering Research Problem
\chapter{Engineering Research Problem}
\setcounter{page}{1}
\label{chap:chap1}

\section{Background }
As the use of Artificial Intelligence (AI) and Machine 
Learning (ML) continues to grow throughout the world 
in high-risk applications, models have become 
ever-increasingly complex and diverse. As a result, 
they often become prone to accidents where unintended 
and harmful behaviour is observed, and consequently are 
scrutinized as disruptive and unreliable solutions. The 
recent emergence in smart cities have seen AI and ML being 
used in various applications such as transportation, 
healthcare, environmental, and public safety as depicted in Figure 1.1.

\begin{figure}[h]
    \includegraphics{smart_city_applications.png}
    \caption{Smart City Artificial Intelligence Applications \cite{chen}}
    \label{fig:smart}
\end{figure}

\enlargethispage{2\baselineskip}
For an AI/ML system to be considered reliable, it must 
perform tasks when required as it was originally 
intended, produce consistent results using real-world 
data (and shifts in data), and remain robust and 
predictable. This means it must also fail in a 
predictable manner \cite{Saif}. 

\section{Applications}
One of the most discussed and disruptive applications of ML is facial 
recognition systems used by authorities which fails to distinguish 
between darker skin individuals. This technology is used to assist the 
police in identifying potential criminals/suspects and often leads to 
wrongful arrests of dark-skinned people \cite{Moutafis}. This example highlights the 
importance and the need for reliability in ML solutions. 

There are many more applications where reliability is crucial due to the 
potential consequences. Cancer diagnosis systems trialled in the US are 
failing to detect cancer in patients in differing hospitals and/or countries which may result 
in death. As another example, unintended behaviours in traffic management systems would 
increase congestion resulting in poor ambient air quality and noise 
pollution.

The rapid technological changes in manufacturing have produced a boom 
in Industry 4.0 applications involving Artificial Intelligence, connected
 devices (IoT) and Big Data. A paper on use cases of AI in Industry 4.0 
 summarises the advantages ML, \textit{“AI with machine learning technique can 
 automate the manufacturing process which increase the productivity, 
 efficiency, optimize production cost and reduce manual error” }\cite{9004327}. A 
 key area is predictive maintenance where real-time equipment data is 
 captured and historical equipment data is evaluated using AI and ML 
 models to estimate the equipment life cycle and hence perform timely 
 maintenance to reduce or eliminate down-time. Down-time is undesirable
for manufacturers as it equates to the loss of revenue.

AI in cybersecurity helps protect enterprises by detecting unusual 
activity, patterns, and malicious behaviour and can respond to different 
situations. For manufacturers, this could be used for asset protection 
while banks and financial institutions may use this form ML to detect suspicious 
activity and fraud \cite{9004327}. 

\section{Project Contextualisation}
A tutorial presented by Suchi Saria and Adarsh Subbaswamy of John Hopkins University \cite{saria2019tutorial} postulates some causes and failure prevention techniques for use in supervised learning systems (regression and classification). 
Some of the sources of unreliability discussed in this tutorial are the use of inadequate data, changes in training and deployment environments, and model misspecification. 
These aforementioned causes will form the basis of this research project.

\enlargethispage{-\baselineskip}
Another reliability issue is discussed in a separate paper, \textit{‘Concrete Problems in AI Safety’} \cite{Amodei} is the prevalence of reward hacking in Reinforcement Learning systems. 
Reward hacking is defined as the AI agent’s ability to cheat the system to achieve the highest reward in an unintended way. 
For example, a positive reward may be given to a traffic management system when there is no congestion. However, the AI model decides to divert all traffic through alternative routes essentially shutting down this particular road/intersection. 
This prevents congestion but does not perform as desired. This notion is also investigated in this research project.

\section{Research Question}
\noindent\rule{\linewidth}{0.4pt}
\begin{quotation}
\textit{How can the reliability of Artificial Intelligence be improved against 
inadequate data labelling, unsuitable algorithm choices, and reward 
hacking?}
\end{quotation}
\noindent\rule{\linewidth}{0.4pt}

%% Methodology
\chapter{Methodology}
This project has been divided into three main sections based on the factors of unreliability mentioned earlier in \hyperref[chap:chap1]{Section 1}. 
The main factors studied in this project are:
\begin{itemize}
    \item Label Bias and Environmental Datashift
    \item Suitable Algorithm Selection
    \item Reward Hacking
\end{itemize}

\section{Label Bias and Environmental Datashift}
To investigate the effect of biased data labelling we will train two models using a single independent algorithm. 
One will use biased data while the other uses unbiased data. 
These datasets will be modelled using the mathematical framework outlined in the conference paper \textit{‘Identifying and Correcting Label Bias in Machine Learning’} \cite{Jiang}. 
Each model will be trained and evaluated using data which has been split from the same distribution. 
For a model to be considered reliable it must be able to properly generalise or adapt well to new and unseen data. 
A good, reliable model can achieve high accuracy scores with low variance between datasets. 
Therefore, both of these trained models will then be fed previously unseen data (i.e. deployment data) to determine its ability to generalize.

\section{Suitable Algorithm Selection}
As can be seen in Figure 2.1, when it comes to AI and ML, the appropriacy of solutions or algorithms depends on elements such as the specific application and the level of supervision required. 
More often than not, more than one algorithm could be a viable solution (see \hyperref[fig:algor]{Figure 2.2}). 
Therefore, to investigate suitable algorithm selections, models will be trained with a single dataset using different algorithms (with different assumptions). 
They will then be tested for accuracy to determine suitable algorithm choices. Evaluating the reliability of a model is dependant of the model type. 
Accuracy, precision and recall are three common metrics we can use to evaluate a model. 
However, depending on certain applications, other complex means of metric evaluation may be necessary. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{ML_algorithms.png}
    \caption{Machine Learning Algorithms for specific applications \cite{Mohapatra}}
    \label{fig:applic}
\end{figure}

The bias-variance trade-off should be considered when optimising ML models. 
Bias is the model’s ability to learn the wrong things due to oversimplification or incorrect assumptions. 
Variance is the error due to sensitivity as a result of small fluctuations in training data. 
As the complexity of the model increases, bias decreases but the variance will increase. 
This is the trade-off between these two factors. 
An overfit model is one that is too complex resulting in high variance and low bias, while an underfit model has low variance and high bias due to its simplistic nature. 
Both overfit and underfit models are undesirable and it is ideal to find a suitable trade-off between bias and variance (hence complexity) to yield a well fit model capable of adapting to different datasets \cite{Jedamski}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{smart_monitoring_algorithms.png}
    \caption{Available ML algorithms for smart monitoring \cite{inproceedings}}
    \label{fig:algor}
\end{figure}

\enlargethispage{\baselineskip}
The dataset/s to be used in the above experiments will be obtained through various open-source data collections available online. 
Therefore, data collection is not a part of this project.
To ensure validity during the training of models, the data distribution will be split into three smaller datasets for training, validation and testing. 
The training set is used to train the models to fit the data and are evaluated against the validations set. 
The validation set being unseen, allows us to determine which models are generalising well to new examples. 
After the best model has been selected it is again tested on the test dataset as a final check on its generalisation ability. 
The training set accounts for 60\% of the full data set, while the validation and test sets account for 20\% each. 

\section{Reward Hacking}
The two unreliability factors discussed in the above experiments are concerned mainly with supervised learning models. 
A major reliability issue within reinforcement learning models is reward hacking. 
We will perform a systematic literature review on applications and known causes of unreliability due to reward hacking as well as potential solutions.

%% Label Bias and Environmental Datashift
\chapter{Label Bias and Environmental Datashift}
Bias is the result of inadequate data where a certain group or class is favoured over another/others hence creating an overrepresentation \cite{Jiang}\cite{saria2019tutorial}.
ML models trained using such datasets will acquire these underlying biases hence making incorrect predictions.

The following mathematical framework can be used as a representation to undestand bias in data \cite{Jiang}

\begin{figure}[h]
    \includegraphics[scale =0.8]{Assumption.png}
    \centering
    \caption{Bias Assumption \cite{Jiang}}
    \label{fig:Assumption}
\end{figure}

In figure \ref{fig:Assumption}, the assumption is that $y_{bias}$ is the label which is closest to $y_{true}$ and achieves the same amount of bias.
In cases where data has been manually manipulated by humans, either consciously or subconsciously, this is deemed to be a reasonable assumption.
The contiguity to $y_{true}$ is given by the KL-divergance, which is used to establish the notion of accurate labeling. 
The Proposition in figure \ref{fig:Proposition} is derived from the KL-divergence. (For complete proof of proposition, see \cite{Jiang})

\begin{figure}[h]
    \includegraphics[scale =0.8]{Prop.png}
    \centering
    \caption{Bias Proposition \cite{Jiang}}
    \label{fig:Proposition}
\end{figure}

Now that $y_{bias}$ is represented in terms of $y_{true}$, we can infer $y_{true}$ in terms of $y_{bias}$ as represented in Figure \ref{fig:Corollary}.

\begin{figure}[h]
    \includegraphics[scale =0.8]{corollary.png}
    \centering
    \caption{Bias Corollary \cite{Jiang}}
    \label{fig:Corollary}
\end{figure}

There may be situations where performance issues may not be apparent during training stages. 
They instead appear post-deployment where training and deployment datasets can have irregularities. 
This is known as Environmental Datashift \cite{saria2019tutorial}. 
This calls into question whether the ML model is robust enough to generalise well to new samples beyond training, or whether it tends to over-generalise to the training dataset thus resulting in unreliability in the real world.


\section{Dataset \& Preprocessing}
The predictive maintenance dataset will be used again to classify failures of an ioT gadget.
During one week, maintenance data was collected from six devices every hour for 168 hrs.
Therefore, this data set contains 1008 rows of data. 
Each cycle of data reading contains the following measurements: 

\section{Results}
\hl{Consider noise in dataset due to uneccassry labels}

Datashift:
\hl{The issue is that modelers typically assume that training data
is representative of the target population or environment where
the model will be deployed} \cite{saria2019tutorial}

\section{Discussion}

%% Suitable Algorithm Selection
\chapter{Suitable Algorithm Selection}
Unreliable Machine Learning models can be the result of inadequate model assumptions where inappropriate or unsuitable algorithm/s have been used.
The appropriacy of an algorithm is dependant on multiple factors. One such factor is the level of supervision required, which in turn is dependant on the amount and type of data available.
Another key factor is the use case of the model and its intended outcomes. Generally model parameters are curated for specific applications and will differ to other use cases.
Therefore, it is important to make use of inductive bias\cite{saria2019tutorial} to when developing reliable models.

\section{Dataset \& Preprocessing}
The predictive maintenance dataset will be used again to classify failures of an ioT gadget.
During one week, maintenance data was collected from six devices every hour for 168 hrs.
Therefore, this data set contains 1008 rows of data. 
Each cycle of data reading contains the following measurements: 

\begin{table}[ht]
    \begin{center}
        \caption{Measurements Dataset} 
        \begin{tabular}{ l|l } 
         \toprule
         \textbf{Measurement} & \textbf{Description} \\  [0.5ex] 
         \midrule
         \textbf{Measurement Time} & Time \\
         \textbf{Gadget ID} & Device number \\
         \textbf{Vibration x sensor} & Horizontal vibration \\ 
         \textbf{Vibration y sensor} & Vertical vibration \\ 
         \textbf{pressure sensor} & Hose pressure \\
         \textbf{Temperature sensor} & Internal temperature \\
         \bottomrule
        \end{tabular}
    \end{center}
\end{table}

The failures dataset contains the precise times each gadget failed. 
During the course of the week, 105 failures were recorded.
Device failure is to be classified when the time remaining to device failure is less than one hour.

This dataset has been split into two datasets for training and testing respectively.
The training dataset will compromise of all data collected from gadget IDs 1-4, leaving data from gadgets 5 and 6 for the test set.
This will ensure the trained models are tested on completely new data. 

For more information on the dataset and use case, please see []
\hyperlink{gitub repo}{https://github.com/Unikie/predictive-maintenance-tutorial}

\section{Algorithms}
The following section shortlists and describes algorithms which can be used to train a supervised model. 
Also discussed are the factors affecting the performance and reliability of these algorithms.

\subsection{Support Vector Machines}
Support Vector Machines (SVM) is a common supervised machine learning algorithm for both classification and regression tasks.
A key attribute of SVMs is its high accuracy and precision in thge segregation of classes.
SVMs create $n$-dimensional hyperplanes to segregate datapoints into $n$ number of classes/groups. 
The algorithm aims to achieve the maximum margin between support vecotrs(closest points), i.e. maximise the minimum margin.

In the case where two classes can be linearly seperated, we consider the following as a representation of a dataset, $S$.
\begin{equation}
    S = \Big\{x_i \in \R^{1 \times p}, y_i \in \{-1, 1\}\Big\}^n_{i=1}
\end{equation}

The values $\{-1, 1\}$ represent two classes of data, $A$ and $B$,
\begin{equation}
    y_i = \begin{cases}
        1, & \text{if $i$} \text{-th sample} \in  A\\
        -1, & \text{if $i$} \text{-th sample} \in  B.
    \end{cases}
\end{equation}

The hyperplane can then be defined as $F_0$ in $\R^D$ space as,
\begin{equation}
    F_0 = \big \{x|f(x) = x \beta + \beta_0 = 0 \big \} 
\end{equation}  
where, 
$\beta \in \R^D$ with norm $||\beta|| = 1$

For a new sample $x^{new}$ which is not within dataset $S$, we can detemine a classification as,
\begin{equation}
    y_{new} = \begin{cases}
        1 \text{(Class A) }, & \text{if } f(x^{new}) > 0 \\
        -1 \text{(Class B) }, & \text{if } f(x^{new}) < 0
    \end{cases}
\end{equation}

The performance of SVMs can be further improved by implementing kernel methods. Some popular kernel functions are:
\begin{figure}[h]
    \includegraphics[scale=0.7]{SVM_kernel.png}
    \centering
    % \caption{Random Forest Algorithm \cite{chen}}
    \label{fig:kernel}
\end{figure}

A linear kernel will be used in this experiment.

%ref
% https://ieeexplore-ieee-org.ezproxy.lib.uts.edu.au/stamp/stamp.jsp?tp=&arnumber=6653952&tag=1
% https://pureadmin.qub.ac.uk/ws/files/17844756/machine.pdf


\subsection{k-Nearest Neighbours}
k-Nearst Neighbours (k-NN) is a simple supervised machine learning algorithm used in both classification and regression problems. 
This approach classifies objects based on the compuatational distances or similarities between samples/values.
The k-NN algorithm only requires tuning of a single parameter, $k$, which represents the amount of nearest samples within the neighbourhood.
The choice of $k$ will affect the algorithm's performance where a value too small would create higher variance hence resulting in less stability.
A larger $k$ value will produce higher bias resulting in lower precision. 

After the number of neighbours, $k$, has been selected, the distances between the query data point, $x_q$, and an arbitrary data point, $x_i$ are to be determined.
Most commonly used is the Euclidean distance (\ref{Euclidean}), however Manhattan distance (\ref{Manhattan}) may also be applied.

\begin{equation} \label{Euclidean}
    d(x_q,x_i) = \sqrt{\sum_{i=1}^{m} (x_q - x_i)^2}
\end{equation}
\begin{equation} \label{Manhattan}
    d(x_q,x_i) = \sum_{i=1}^{m} \lvert x_r - x_i \rvert
\end{equation}

The resulting values are then sorted by distance from smallest to largest and the first $k$ entries are selected. 
In classification problems, the mode of $k$ labels is returned, while the mean of $k$ labels is returned in regression problems.
The choice of parameter $k$ can impact the models reliability. Too small a value for $k$ will result in higher variance while a value too high will increase bias. 
This can be problematic when used on noisy datasets. Cross validation can be used to determine the correct $k$ value.

% references:
% https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761
% https://www.listendata.com/2017/12/k-nearest-neighbor-step-by-step-tutorial.html
% http://www.lkozma.net/knn2.pdf
% https://ieeexplore-ieee-org.ezproxy.lib.uts.edu.au/stamp/stamp.jsp?tp=&arnumber=7456223

\subsection{Random Forest}
Random Forest is an ensemble machine learning method which creates multiple random decision trees and combines their respective votes (classification) or averages (regression) to improve prediction accuracy and fitting.

% \enlargethispage{3\baselineskip}
\begin{figure}[h]
    \includegraphics[scale=0.7]{random_forest.png}
    \centering
    % \caption{Random Forest Algorithm \cite{chen}}
    \label{fig:RF}
\end{figure}

Add more stuff about reliability/performance factors

% Ref:
% https://www.math.mcgill.ca/yyang/resources/doc/randomforest.pdf

\subsection{Neural Networks}
Some filler text for the time being.

\section{Results}
Building on top of the predictive maintenance project [], we train models using the following algorithms and test for Precision, Recall, Accuracy, and AUC scores.
The results are depicted in Figures \ref{fig:Metric plot}, \ref{fig:AUC}, and Tables \ref{table:Metrics}, \ref{table:Conf}
\begin{itemize}
    \item Random Forests (RF)
    \item Logarithmic Regression
    \item Linear Regression
    \item k-Nearest Neighbours (knn)
    \item Neural Networks (nn)
    \item Support Vector Machines (SVM)
    \item Naive Bayes
    \item Stochastic Gradient Descent (SGD/clf)
\end{itemize}

\begin{figure}[H]
    \includegraphics[scale=0.9]{results.png}
    \centering
    \caption{Precision, Recall and Accuracy Scores}
    \label{fig:Metric plot}
\end{figure}

\begin{figure}[H]
    \includegraphics[scale=0.9]{AUC.png}
    \centering
    \caption{Area Under the Curve}
    \label{fig:AUC}
\end{figure}

\begin{table}[h]
    \caption{PDM Classification Metrics}
    \label{table:Metrics}
    \begin{tabular}{lrrrrr}
        \toprule
            \textbf{Algorithm} &  \textbf{Precision} &  \textbf{Recall} &  \textbf{Accuracy} &       \textbf{AUC} &        \textbf{F1} \\
        \midrule
        \textbf{Random Forest} &   0.310680 &  \cellcolor{lightgray} 1.0000 &  0.782875 &  \cellcolor{lightgray}0.879661 &  \cellcolor{lightgray}0.474074 \\
        \textbf{Logarithmic Regression} &   0.300000 &  0.9375 &  0.779817 &  0.850106 &  0.454545 \\
        \textbf{Linear Regression} &   0.000000 &  0.0000 &  \cellcolor{lightgray}0.902141 &  0.500000 &  0.000000 \\
        \textbf{k Nearest Neighbours} &   0.285714 &  0.0625 &  0.892966 &  0.522775 &  0.102564 \\
        \textbf{Neural Network} &   \cellcolor{lightgray}0.363636 &  0.1250 &  0.892966 &  0.550636 &  0.186047 \\
        \textbf{SVM} &   0.303030 &  0.9375 &  0.782875 &  0.851801 &  0.458015 \\
        \textbf{Naive Bayes} &   0.303030 &  0.9375 &  0.782875 &  0.851801 &  0.458015 \\
        \textbf{CLF} &   0.104869 &  0.8750 &  0.256881 &  0.532415 &  0.187291 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h]    
    \centering
    \caption{PDM Confusion Matrix}
    \label{table:Conf}
    \begin{tabular}{lrrrr}
        \toprule
        \textbf{Algorithm} &   \textbf{TP} &   \textbf{FP} &  \textbf{FN} &  \textbf{TN} \\
        \midrule
        \textbf{Random Forest} &  224 &   71 &   0 &  32 \\
        \textbf{Logarithmic Regression} &  225 &   70 &   2 &  30 \\
        \textbf{Linear Regression} &  295 &    0 &  32 &   0 \\
        \textbf{k Nearest Neighbours} &  290 &    5 &  30 &   2 \\
        \textbf{Neural Network} &  288 &    7 &  28 &   4 \\
        \textbf{SVM} &  226 &   69 &   2 &  30 \\
        \textbf{Naive Bayes} &  226 &   69 &   2 &  30 \\
        \textbf{CLF} &   56 &  239 &   4 &  28 \\
        \bottomrule
    \end{tabular}
\end{table}

In Table \ref{table:Metrics}, highlighted are the highest comparison scores between tested algorithms.
The ensemble method Random Forests is evidently the most suited method to this predictive maintenace application.
Linear Regression delivered the highest accuracy meaning it was able to make correct predictions the best. 
However, it did not perform well in the other metrics. This highlights that algorithm evaluation cannot be determined based on accuracy alone.

To ensure reliability and robustness, the goals of the use case should be considered and may contradict performance metrics.
In this scenario, a True Positive (TP) case where the predicted failures are actual failures should take precedence as this is the money saving factor. 
A False Negative (FN) case occurs when the model predicts a non-failure but in reality a failure has occured. This would result in large financial penalties for the company due to downtime.
Therefore, the highest amount of TP cases and lowest amount of FN cases are desired.

\section{Further Research}

A seperate study compares existing resgression based machine learning algorithms on a similar but more complex predictive maintenance application,
\textit{'Prediction of Remaining Useful Lifetime (RUL) of Turbofan Engine using Machine Learning'} \cite{RUL}.

During this study, models were trained on a dataset obtained by NASA's data repository, where turbofan jet engines are run until failure.
During operation 21 sensor measurements are recorded against a time series per engine. Training and evaluation occurs on four datasets, each containing data from 100-250 engines. 
This dataset is much more complex than the one used in the previous experiments,
hence it is expected to see much more variance in accuracy of the tested algorithms. 

Listed below are machine learning algorithms used to predict RUL in this study:
\begin{itemize}
    \item Linear Regression
    \item Decision tree
    \item Support Vector Machine
    \item Random Forest
    \item K-Nearest Neighbours
    \item The K Means Algorithm
    \item Gradient Boosting Method
    \item AdaBoost
    \item Deep Learning
    \item Anova 
\end{itemize}
\bigskip

Like the Random Forest method, Gradien Boosting, AdaBoost, and Anova are also ensemble methods.


\begin{table}[h]
    \bigskip
    \caption{RUL Algorithms Root Mean Square Error values\cite{RUL}}
    \begin{figure}[H]
        \includegraphics[scale=0.4]{RUL_results.png}
        \centering
    \end{figure}
    \label{table:RMSE}
\end{table}

\begin{figure}[H]
    \caption{RUL Algorithms Root Mean Square Error Plots per data set\cite{RUL}}
    \includegraphics[scale=0.4]{RUL_RMSE.png}
    \label{fig:RMSE Plots}
\end{figure}

Table \ref{table:RMSE} and Figure \ref{fig:RMSE Plots} displays the RMSE values of all tested algorithms across all four datasets.
As can clearly be seen, the complexity of this problem results in fluctuation of evaluation scores unlike our previous classification experiment.
RMSE is a measure of the concentration of data around the line of best fit. Therefore, an algorithm with the lowest RMSE value is desired.
In this application, the random Forest algorithm produced the lowest error value.

When comparing these results to those of our previous classification experiement, it is clear that enseble methods perform the best, specifically, Random Forest. 
It should also be noted that SVM outperformed knn in the classification experiement but performed worse in the RUL regression study.


\bigskip
\hl{add another study}

\section{Recommendations}
Above sections highlight the importance of correct model spcifications. Previous assumptions and expectations may not always be correct.

\hl{Use ensemble methods and hyperparameter tuning.

It is also important to select appropriate evaluation metrics based on the model type and application.
Assign a weight or cost to each output of the confusion matrix

Research specific use case scenario to find suitable algorithms through proven research}
% https://pdf.sciencedirectassets.com/271506/1-s2.0-S0957417417X00109/1-s2.0-S0957417417302397/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEF4aCXVzLWVhc3QtMSJGMEQCIC53FsqmSpjp%2BQZj9Uvlsnq4pWNYncx8FF2OQ23FmKgkAiAcXvDwMGwRjH0UYxFOV3NaELWAqNC04I378835HV8Ddyq9Awi3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAMaDDA1OTAwMzU0Njg2NSIMYqA70v0TzrNak19iKpEDBvqMBBuWJ0aYNJouVwXQ35CXZg6ARpd5zZsgttN8QGD%2B3j4NV2v289xxiDQQNfltw6POyIApxTgYr6jp4s5S5FwYqdYy6xIZDl7XbC7IY7chzx3bV5bs%2BdRhZcW8WJPnHAjuH6yC6%2BtUY8e7nk5k2WYsdLfkV8Tk2qYpnSqkjyK1cCTM55SOwbgcNJ0w%2Fpo8PJq3mdzi8kI9fD9DFv8wVsDUs0dRXp2jPtTTduTIhNwGa4JgLTh1xplV23kKewbIalHtdNFiqPZB%2Bbr%2FITjQSbWAlwvorEiq31BbESWXfTIIlxQdixRt20XOCypDeG2RrsBe3jPt3t3DBVNF4%2BOkNE2%2BaWmbtgcekVFXZEsTpRRMirynYAJ3FKpj%2BRck%2FKKxUQ9UjqhrMkQrDvXfcqo5QrWlvI4uGPiNoec5Z%2F6bRMXdDgDWP%2FvXQ6Q86YA%2BOLvbiNWgQTfmUFYT8fFgsljtVqiov7W%2BRNWA2Vj%2F%2BKsxtdc7Ok%2BOhxSE%2F9JS%2BSemJuBxJmSlbQqOr8DPQHYlp6h4YxMwl8DPgwY67AGgOpD6YmdbpnxG6nEHk1pw57wlOHkaF2iNo4FC9KM8jegDXaKv0LtK3bBsAigrskq4QKpZXpaAopbnKphKhfoOYrdmgIm7Ba0V44nNPSe46I5%2FsBikWMh3ZQ6s7lqBT1EQa4ds5NQAhi26Vf02PMgLXLk67acqUys0Go%2FKgQv2reuep1GgeOilitWRdjJwkwe07XUnJbNGMMcYNVvqgn%2FikKG4gZzshEt142pOV5N2lNfJbn7GleRVzP6u5NW1NwKmnbDm50JZ5iGgFV3pOpErKyuMC8JoEc3vDhF35jJ6tITkIHuvGvxGACX4iw%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210412T063232Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6KJJLFFB%2F20210412%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=3a49f5dce9775617b98bcc583609d05d8482af9edf219e50219977b74ba83b62&hash=8a44df99adabed4a00d5099a66fae6484d6654bda9d1402f88f056bc364a2330&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0957417417302397&tid=spdf-75df757c-2550-4170-8694-4099a14ded3c&sid=c7d6cc998d9f344cac299b155f55848b59cbgxrqa&type=client
% https://www-sciencedirect-com.ezproxy.lib.uts.edu.au/science/article/pii/S0360835219304838?via%3Dihub

%% Reward Hacking
\chapter{Reward Hacking}
Unlike Supervised and Unsupervised Learning methods where predictions are made on underlying class types or patterns respectively, 
Reinforcement Learning operates on a reward-action (trial and error) system where feedback is provided to determine the most optimatal method to perform an action.
As the agent takes actions to ineteract and manipulate the environment, feedback, $S_t$, and a reward fucntion, $R_t$, are returned from the environment. 
The Agent is then able to iterate through another cycle by performing another action based on the feedback and rewards recieved.

\begin{figure}[H]
    \centering
    \caption{Reinforcement Learning and the Environment \cite{amiri_mehrpouyan_fridman_mallik_nallanathan_matolak_2018}}
    \includegraphics[scale=0.5]{RL.png}
    \label{fig:RL}
\end{figure}

The Markov Decision Processes (MDP) can be used to represent an RL framework per time steps.
MDP is represented as a tuple, $(S, a, P, R, \gamma)$ where:
\begin{itemize}
    \item $S$ is a set of states,
    \item $a$ is a set of actions,
    \item $P$ is the state trainstion probability matrix
    \item $\gamma$ is a discount factor, 
    \item $R$ is a reward function 
\end{itemize}   

An action, $a_t$, taken under state, $S_t$ leads to the following state, $S_{t+1}$. 
\begin{equation}
    s_t \xrightarrow{a_t} s_{t+1}
\end{equation}

A reward, $R_t$, is then awarded for the current state based on the actions.
\begin{equation}
    R_t(s_t, a_t) 
\end{equation}

However, RL agents are slso succeptible to unreliabilties. The phenomenon of reward hacking occurs when the agent is able to maximise its reward in an unintended and undesirable way hence "is gaming the system".

\section{Applications}
RL approaches can also be applied to predictive maintenance applications to reduce critical downtime. 
As discussed earlier in Section 4.4, regressional models (as well as deep learning) are able to accurately make predictions on RUL for example. 
Nonetheless, such methods are unable to provide meaningful information or observations to aid the decision making process of maintenance operations due to complexities or unknowns in the environment \cite{9221098}.

An example of this RL scenario \cite{9221098}, where the objective function was to maximise the sensor lifetime.
The reward function (\ref{eq:reward fucntion}) was defined as such to manage the agent's actions:
\begin{equation}
    \label{eq:reward fucntion}
    R_t = \begin{cases}
        R_{Rpl}, & \text{if } S_t^i > 0, \beta > 0\\
        R_{Rpa}, & \text{if } S_representedt^i > 0, \beta > 0\\
        R_{Exp}, & \text{if } S_t^i > 0\\
        R_{Frug}, & \text{if } S_t^i > 0, \beta > 0\\
        R_{Pen}, & otherwise
    \end{cases}
\end{equation}

Another application of RL approaches is within traffic management systems (TMS). 
Traffic congestion increases environmental and noise pollution, fuel consumption, as well as operating costs.
This study \cite{JOO2020324} aims to maximise the amount of vehicles through intersections while minimising standard deviation of traffics jams.
The following reward function was used to assess the agent's actions:
\begin{equation}
    r_t = log_\delta(f(t))
\end{equation}
\begin{equation}
    f(t) = \alpha \cdot (d_{ql}) + (1 - \alpha)\cdot(\tau^{tp})
\end{equation}

In order to exploit these reward functions, agents may start to behave irrationally. 
For example, the TMS may choose to give precedence to a certain queue within an intersection, or it may completely completely ignore the standard deviation if the throughput in a particular direction provides a better reward.

Similarly, if a higher reward is given to the predictive maintenance agent if it applies a 'repair' or 'replace' action, it may continuously flag a fault even though no faults exist. 

\section{Causes of Unreliability}
The following sections describe some common causes of Reward hacking.
There is no one single cause or action that can cause an agent to game its reward system. 
Often a system may encounter a combination of these problems.

\hl{Find some more if needed}

\subsection{Poor Feedback Loops}
If the objectivive function includes a parameter such as discount, $\gamma$, which can boost or diminish itself, another parameter becomes redundant.
As a result, the objective function no longer behaves as it was intended.
The example provided in the previous section is a great example of this. The standard deviation was intended to equalise the importance of all queues.
But it may be diminished by the strong feedback loop of vehicles passing through the intersections.

\subsection{Goodhart's Law}
\begin{quotation}
    \textit{"When a measure becomes a target, it ceases to be a good measure."}
\end{quotation}
If the interelationship betwen an objective function and its means or factors to successfully accomplishing a task is too high,
that relationship will not hold under heavy optimisation. 
Continuing on with our TMS example, if the success of the operation were only based on throuput of vehicles across an intersection,
the agent may choose to show a certain queue the green light forever. 
This would mean there is no queue of vehicles at this intersection and the agent would keep collecting the maximum reward for keep the queue count at zero.

\subsection{Wireheading \& Reward Function Tampering}
Wireheading is ability of an intelligent agent to modify its reward sensors to ensure it always recieves the maximum reward \cite{JOO2020324}. 
In short, Wireheading occurs if the agent is no longer optimisising the the objective function, but rather assumes control of the measuring system (reward process).
Reward Function Tampering is defined as the agent's ability to modify or rewrite its reward function to yield maximum reward.
This encouragement to game the reward function results in a deviation from the intended goal/s. 
At the present time, most RL agents are not yet intelligent enough to cause serious havoc in most applications however, some examples of Wireheading/Reward Tampering are presented in \cite{DBLP:journals/corr/abs-1908-04734}.  
Concern arises in cases where humans have influence in the reward process (implicityly or explicitly) and can be a danger to themselves and others.

\subsection{System Complexity}
As the complexity of a program or system increases, so does the likelihood of bugs and glitches existing within the code.
Consequently, the chance of the reward system being taken advantage of increases. 
Despite the fact that such issues can be easily rectified, a simple bug or sesnor fault in traffic cameras/sensors  for a TMS system can and often will be taken advantage of by the agent. 


\section{Approaches for Prevention}
\subsection{Careful Engineering}
In most cases, the simplest yet most tedious solution to reward hacking is careful engineering.
Performing formal verification or thorough practical testing is an easy way to iron out issues which may cause problems down the line.
Cyberscuirty approaches such as sandboxing could also be beneficial as a means to segregate the agent from rewards.

\subsection{Reward Strategies}

\subsubsection{Reward Capping}
To prevent the agent performing high payoff and low probability approaches, reward capping can be implemented.
This would disclose to the agent that performing these undesired actions will no longer provide a reward which is significantly larger than optimising the function in the desired manner.

\subsubsection{Multiple Rewards}
Multiple rewards functions can be used and combined by averaging or further optimisation to deter reward hacking.
Each reward function could differ slightly, mathematically or by objective function, ensuring that an invulnerability in one reward function does not impact the whole system.
Although it is unlikely, there is a chance that all reward functions could still be hacked.

\subsubsection{Inverse Rewards}
When an agent performs an action which takes a step towards the correct direction in terms of objective function, it is given a reward.
The inverse could be applied as well. If the agent takes actions which move in the opposite direction to the objective function, it can be given a negative reward.
\hl{more maths}

\subsubsection{History Based Rewards}
The following claim has been made in \cite{DBLP:journals/corr/abs-1908-04734}:
\begin{quote}
    \emph{''A history-based reward function exists that avoids the RF-input tampering
    problem, if a deterministic (history-based) policy exists that reliably performs the task.''}
\end{quote}

\subsubsection{Belief Based Rewards}

\subsection{Model Lookahead}
A viable solution to prevent the model/agent from replacing its reward function (Reward Tampering) is to implelement Model Lookahead.
A model based RL system uses a model to develop a strategy for future actions by assessing the states a seiries of actions would lead to.
A reward can be provided according to the model's anticipated state/s rather than providing a reward with respect to the current state.
The agent will not be able benefit from hacking as these exploitations will most likely not reach the anticipated states.
To represent Mathematically:
\begin{align*}
    \text{current reward function: } &\quad\theta^R_k \\
    \text{simulated future trajectories } &\quad  k_{k+1},... S_m  \\
    \text{at time } k, &\quad  R_t^k = R(S_t;\theta^R_k)
\end{align*}
    
This notion continues in a similar study (see for complete mathematical breakdown) \cite{EverittFDH16} where three agent definitions are proposed, in Figure \ref{fig:value_func} and Table \ref{table:val_func_table}.
Hedonistic value functions are calculated using the future utility function, $u_{t+1}$, instead of the current utility function, $u_t$.
Therefore, these value functions promote self modification.
The modification of utility functions for Ignorant and Realistic value functions only affect modifications of future versions of that model.

\begin{figure}[H]
    \centering
    \caption{Self-modification value functions \cite{EverittFDH16}}
    \includegraphics[scale=0.8]{lookahead_definitions.png}
    \label{fig:value_func}
\end{figure}

\begin{table}[h]
    \bigskip
    \caption{Self-modification value functions \cite{EverittFDH16}}
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.8]{lookahead_table.png}
    \end{figure}
    \label{table:val_func_table}
\end{table}

\subsection{Adversarial Methods}

\subsection{Trip Wires}

%% Conclusion
\chapter{Conclusion}

%% Bibliography
\renewcommand{\bibname}{References}
\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}